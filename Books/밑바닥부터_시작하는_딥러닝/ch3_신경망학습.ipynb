{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 요약\n",
    "\n",
    "1. 신경망 학습은 최적의 매개변수를 자동으로 갱신하는 것을 목표로 한다\n",
    "2. 최적의 매개변수는 손실 함수의 값이 최소가 되게 하는 값이다\n",
    "3. 경사법은 기울기로 손실 함수가 최소가 되는 방향을 알려준다\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 배경\n",
    "\n",
    "신경망의 매개변수는 기본적으로 수만개로 시작한다. 때문에 데이터에서 매개변수의 가장 적절한 값을 자동으로 찾는 방법이 필요하다. 신경망 학습에서는 이 방법의 지표로 손실 함수를 만들게 된다. 학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것이다. 기존의 신경망에서 매개변수를 수동으로 입력해야하는 번거로움을 신경망 학습으로 자동화 하게 된다.\n",
    "\n",
    "신경망 학습의 지표인 손실 함수에 입력값을 기반으로 출력값을 가장 작게 만드는 기법으로 함수의 기울기를 사용한다.\n",
    "\n",
    "## 데이터 주도 학습의 변화\n",
    "\n",
    "기계학습의 핵심은 데이터이다. 신경망 학습의 가장 큰 장점은 기계가 수집한 데이터에서 패턴 찾는 것을 시도하고, 데이터에서 답을 찾는 과정에서 사람이 배제된 것에 있다.\n",
    "\n",
    "하지만, 기계학습 단계에서는 여전히 사람의 손이 필요하다. 공통된 특징을 가진 데이터를 사람이 설계한 변환기에 넣고 특징의 본질을 의미하는 데이터를 추출하는 것이다. 보통 입력 데이터는 벡터로 변환하여 사용하게 되고 이 변환기를 사람이 만들게 된다.\n",
    "\n",
    "신경망(딥러닝)에서는 데이터의 특징을 추출하는 변환기까지 기계에게 학습 시키는 것으로, 입력 데이터의 패턴을 분석하고 결과 출력까지 사람의 개입이 없다.\n",
    "\n",
    "## 훈련 데이터와 시험 데이터\n",
    "\n",
    "기계학습의 최종 목표는 범용적으로 사용 가능한 모델을 획득하는 것이다. 이를 위해 데이터를 훈련 데이터와 실험 데이터로 분리하고, 훈련 데이터로 학습한 모델의 범용 능력을 평가를 위해 시험 데이터로 모델의 범용성을 실험한다.\n",
    "\n",
    "여기서 훈련 데이터의 학습을 통해 최적의 매개변수를 획득하게된다. 학습 데이터의 종류는 반드시 임의로 제공된 공통된 특징을 갖고 있는 데이터이어야 오버피팅 상태를 회피 할 수 있다.\n",
    "\n",
    "## 손실 함수\n",
    "\n",
    "### 오차제곱합\n",
    "\n",
    "오차제곱합의 수식의 결과값이 낮을 수록 정답에 가까울 확률이 높다는 것을 의미한다\n",
    "\n",
    "- Code\n",
    "\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    ## y = 소프트맥스 함수의 출력값\n",
    "    y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0 ] # 정답이 2일 확률이 60%를 표현\n",
    "\n",
    "    ## t = 원-핫 인코딩 표기법: 하나의 원소만 1이고 나머지는 0으로 나타내는 표기법\n",
    "    t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ] # 1원소가 2번 인덱스 이므로 정답이 2인것을 표현\n",
    "\n",
    "    ## 오차제곱합의 수식\n",
    "    ### 오차제곱합의 수식의 결과값이 낮을 수록 정답에 가까울 확률이 높다는 것을 의미한다\n",
    "    def sum_squares_error (y, t):\n",
    "        return 1 /  * np.sum( (y - t) ** 2 )\n",
    "\n",
    "    sum_squares_error(np.array(y), np.array(t)) # -> 0.09750000000000003\n",
    "\n",
    "    y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0 ] # 정답이 7일 확률이 60%를 표현\n",
    "    sum_squares_error(np.array(y), np.array(t)) # -> 0.5975\n",
    "    ```\n",
    "\n",
    "\n",
    "### 교차 엔트로피 오차\n",
    "\n",
    "오차제곱합과 마찬가지로 결과값이 작을수록 정답일 확률이 높다는 것을 의미한다.\n",
    "\n",
    "- Code\n",
    "\n",
    "    ```python\n",
    "    ## 교차 엔트로피 오차\n",
    "    ## 오차제곰합과 마찬가지로 결과값이 작을수록 정답일 확률이 높아진다는 의미\n",
    "    ## 수식에는 없는 델타값은 더한 이유: np.log() 함수에 0을 입력하면 -inf(음의 무한대)가 되어 계산이 불가능해지기 때문에\n",
    "    ##                          아주 작은 값을 더해 음의 무한대 값을 피한다\n",
    "    def cross_entropy_error(y, t):\n",
    "        delta = 1e-7\n",
    "        return -np.sum( ( t * np.log(y + delta) ) )\n",
    "\n",
    "    y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0 ]\n",
    "    t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ]\n",
    "    cross_entropy_error(np.array(y), np.array(t)) ## -> 0.510825457099338\n",
    "\n",
    "    y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0 ]\n",
    "    cross_entropy_error(np.array(y), np.array(t)) ## -> 2.302584092994546\n",
    "    ```\n",
    "\n",
    "- 손실함수는 신경망의 학습 상태를 지표이다\n",
    "\n",
    "손실 함수는 기본적으로 신경망 학습의 현재 상태가 얼마나 안 좋은지를 표현한다. 기계학습에서는 이 손실 함수의 값을 최대한 작은 값으로 만드는 매개변수를 탐색하는 용도로 사용하게 된다. 일반적으로 손실 함수로 오차제곱합과 교차 엔트로피 오차 함수를 사용한다.\n",
    "\n",
    "### 평균 손실 함수\n",
    "\n",
    "빅데이터의 훈련 데이터는 보통 데이터가 수백만개이고 손실 함수를 구하는 것에도 시간이 들게 된다. 이런 경우에는 손실 함수의 값을 구한느 것에도 시간이 많이 들기 때문에 근사치를 구하기 위해 평균 손실 함수를 구하여 이를 사용하게 된다.\n",
    "\n",
    "### 미니 배치 학습\n",
    "\n",
    "미니 배치 학습은 훈련 데이터의 일부를 가지고 신경망 학습을 하는 것을 말한다.\n",
    "\n",
    "### 손실 함수가 필요한 이유\n",
    "\n",
    "신경망을 이루는 퍼셉트론의 요소 중 가중치와 임계값을 매계변수라고 한다. 기계학습에서 신경망 학습은 이 매계변수가 최적의 값을 갖도록 하는 것을 목표로 하고 있다. 이를 위해 손실 함수를 사용하여 최적의 매계변수를 서서히 갱신하는 과정을 반복하게 된다. 손실함수의 값을 최대한 작게 만드는 매계변수가 최적의 매계변수이기 때문에 우리는 미분이라는 수학적 개념을 사용하게 된다.\n",
    "\n",
    "## 수치 미분\n",
    "\n",
    "극한을 사용하는 미분과 달리 수치 미분은 근사치로 미분 계산한다. 이로인해 반올림 오차가 발생하지만 이를 개선하기 위해 $10^{-4}$ 로 오차를 설정하여 계산한다.\n",
    "\n",
    "### 편미분\n",
    "\n",
    "```python\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "\n",
    "    return grad\n",
    "```\n",
    "\n",
    "변수가 여러개인 함수에 대해 각 변수별로 따로 미분하는 것을 편미분이라고 한다\n",
    "\n",
    "## 기울기 ( Gradient )\n",
    "\n",
    "모든 변수의 편미분을 동시에 진행하여 벡터로 정리한 것을 기울기라고한다. 기울기가 가리키는 방향은 각 장소에서 출력 값을 가장 크게 줄이는 방향이다.\n",
    "\n",
    "### 경사법\n",
    "\n",
    "$x_0 = x_0 - \\eta { df \\over dx_0}$    $(단, \\eta = 학습률, f = 손실함수, x_0 = 매개변수)$\n",
    "\n",
    "기계학습의 대부분의 문제는 손실 함수의 값을 최소화하는 매계변수를 찾아내는 것이다. 기울기는 이러한 최소값을 찾기 위해 이용되며 이를 경사법이라고 한다. 하지만, 경사법은 단순하게 기울기가 0이되는 지점의 기울기를 나타내기 떄문에 해당 방향이 입력된 손실함수의 최소값이라는 보장이 없다. 이를 위해 경사법으로 나온 기울기가 가리키는 방향으로 조금씩 이동하면서 반복적으로 기울기를 구하면서 최소값을 갱신해야한다\n",
    "\n",
    "- 경사법에서 학습률 변수는 하이퍼 파라미터로 분류하며, 학습률을 수정하면서 해당 신경망 학습이 올바르게 되고 있는지 확인한다\n",
    "\n",
    "![스크린샷 2022-10-06 오후 1.12.53.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1d2acda9-77d8-4592-a223-6b14c6829037/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-10-06_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.12.53.png)\n",
    "\n",
    "![스크린샷 2022-10-06 오후 2.36.05.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/165f5bf5-fe3a-41ae-a1c9-632b58c68d89/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-10-06_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.36.05.png)\n",
    "\n",
    "### 신경망에서의 기울기\n",
    "\n",
    "- ${ dL  \\over dW }$ 손실 함수 $L$에 대하여 가중치 매계변수  $W$에 대한 미분값"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 오차제곱합 실습\n",
    "##### 오차제곱합의 수식의 결과값이 낮을 수록 정답에 가까울 확률이 높다는 것을 의미한다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "0.11949999999999998"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## y = 활성화 함수(소프트맥스)의 출력값\n",
    "y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0 ] # 정답이 2일 확률이 60%를 표현\n",
    "\n",
    "## t = 원-핫 인코딩 표기법: 하나의 원소만 1이고 나머지는 0으로 나타내는 표기법\n",
    "t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ] # 1원소가 2번 인덱스 이므로 정답이 2인것을 표현\n",
    "\n",
    "def sum_squares_error (y, t):\n",
    "    n = len(y)\n",
    "    sum = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        sum += np.sum( (y[i] - t[i]) ** 2 )\n",
    "\n",
    "    return sum / n\n",
    "\n",
    "sum_squares_error(np.array(y), np.array(t)) # -> 0.09750000000000003\n",
    "\n",
    "y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0 ] # 정답이 7일 확률이 60%를 표현\n",
    "sum_squares_error(np.array(y), np.array(t)) # -> 0.5975\n",
    "\n",
    "##### N개의 훈련 데이터를 오차제곱합의 수식 ######"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 교차 엔트로피 오차 실습\n",
    "##### 오차제곰합과 마찬가지로 결과값이 작을수록 정답일 확률이 높아진다는 의미\n",
    "##### 수식에는 없는 델타값은 더한 이유: np.log() 함수에 0을 입력하면 -inf(음의 무한대)가 되어 계산이 불가능해지기 때문에 아주 작은 값을 더해 음의 무한대 값을 피한다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4288846466.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [110]\u001B[0;36m\u001B[0m\n\u001B[0;31m    sum += -np.sum( ( t * np.log(y + d elta) ) )\u001B[0m\n\u001B[0m                                       ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cross_entropy_error(y, t):\n",
    "    n = len(y)\n",
    "    sum = 0\n",
    "    delta = 1e-7\n",
    "\n",
    "    for i in range(n):\n",
    "        sum += -np.sum( ( t * np.log(y + d elta) ) )\n",
    "\n",
    "    return sum / n\n",
    "\n",
    "y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0 ]\n",
    "t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ]\n",
    "cross_entropy_error(np.array(y), np.array(t)) ## -> 0.510825457099338\n",
    "\n",
    "y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0 ]\n",
    "cross_entropy_error(np.array(y), np.array(t)) ## -> 2.302584092994546"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 해석적 미분"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 순간 변화량을 표현하는 해석적 미분\n",
    "def numerical_diff(f, x):\n",
    "    ## 미분 에서 lim을 표현하기 위해 0보다 큰 가장 작은 값을 넣는다\n",
    "    ## 하지만, 이로 인해 계산의 결과에서 반올림 오차만큼의 오차가 발생하게 된다.\n",
    "    ## 이를 개선하기 위해 10^(-4)의 값을 h로 할당한다\n",
    "    h = 1e-4\n",
    "    m = (f(x + h) - f(x - h)) /(2 * h)\n",
    "    return m\n",
    "\n",
    "def b_numerical_diff(f, x):\n",
    "    h = 0.0001\n",
    "    return ( f(x + h) - f(x) ) / ( 2 * h )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def func_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "## retrun: x에 대한 f(x)의 변화량 -> 해석적 미분의 실제 기울기 값\n",
    "print(numerical_diff(func_1, 5))\n",
    "\n",
    "## return: x에 대한 f(x)의 변화량 -> 해석적 미분의 실제 기울기 값\n",
    "print(numerical_diff(func_1, 10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 편미분 실습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def func_2(x):\n",
    "    return np.sum(x ** 2) ## x1^2 + x2^2\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0 * x0 + 4.0 ** 2.0\n",
    "\n",
    "def numerical_gradient(f, x): ## 기울기 구하기\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) ##\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "print(numerical_gradient(func_2, np.array([ 3.0, 4.0 ])))\n",
    "\n",
    "print(numerical_gradient(func_2, np.array([ 0.0, 2.0 ])))\n",
    "\n",
    "print(numerical_gradient(func_2, np.array([ 3.0, 0.0 ])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "X = np.arange(-6, 6, 0.1)\n",
    "Y = np.arange(-6, 6, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = np.sqrt(X ** 2, Y ** 2)\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='summer')\n",
    "ax.contourf(X, Y, Z, zdir='z', offset=0, cmap=plt.cm.summer)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 미니배치 학습 실습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "## 기울기 함수\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "\n",
    "        return grad\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "\n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "\n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(func_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 경사 하강법"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 경사 하강법 함수\n",
    "def gradien_decent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x) ## 기울기 함수\n",
    "        x -= lr * grad ## 매계변수 - ( 학습률 * 기울기 )\n",
    "\n",
    "    return x, np.array(x_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## 경사법으로 f = x1^2 + x2^2의 최솟값을 구해라\n",
    "def func_2(x): ## x = [ x1, x2 ]\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([ -3.0, 4.0 ])\n",
    "x, x_history = gradien_decent(func_2, init_x, lr = 0.1)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 신경망에서의 기울기 구하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "## 활성화 함수\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "## 손실 함수\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "## 기울기 함수\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) ## 정규분포 초기화 -> 랜덤 행렬\n",
    "\n",
    "    ## 입력된 x와 랜덤으로 생성된 가중치 행렬과의 곱으로 y값을 내보낸다\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    ## 손실 함수 ( 교차 엔트로피 오차 )\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) ## 입력신호 * 가중치\n",
    "        y = softmax(z) ## 활성화 함수를 통과한 확률 배열이 나온다\n",
    "        loss = cross_entropy_error(y, t) ## 손실 함수 통과\n",
    "        return loss ## 손실 함수의 결과값"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 신경망 생성\n",
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "X = np.array([ 0.6, 0.9 ])\n",
    "p = net.predict(X) ## 랜덤으로 생성된 가중치와의 곱으로 생성된 y값 행렬\n",
    "print(p)\n",
    "\n",
    "max = np.argmax(p) ## 결과 배열 중 최대값 요소의 인덱스 출력\n",
    "print(max)\n",
    "\n",
    "t = np.array([ 0, 0, 1 ]) ## 정답 레이블 생성\n",
    "net.loss(p, t) ## 손실 함수 통과 -> 0.928 ~"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def f(W):\n",
    "#     return net.loss(x, t)\n",
    "f = lambda w: net.loss(x, t) ## 간단한 표현식\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "## 해당 가중치 매개변수를 줄이기 위해서는 해당 기울기가 양수이면 음의 방향으로, 음수이면 양의 방향으로 이동해야함을 알 수 있다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 학습 알고리즘 구현 실습\n",
    "- 1단계: 미니 배치\n",
    "- 2단계: 기울기 산출\n",
    "- 3단계: 매개변수 갱신\n",
    "- 4단계: 1 ~ 3단계 반복"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 2층 신경망 생성\n",
    "# from common.functions import *\n",
    "# from common.gradient import numerical_gradient\n",
    "import numpy as np\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size) ## 인풋 행렬, 히든 행렬 사이즈 중간의 가중치 행렬 생성\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size) ## 은닉층 편향 행렬 생성\n",
    "\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size) ## 은닉층 행렬, 출력층 행렬 중간 사이즈 행렬 생성\n",
    "        self.params[\"b2\"] = np.zeros(output_size) ## 출력층 편향 행렬 생성\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        ## x = 입력 데이터\n",
    "        ## t = 정답 레이블\n",
    "        y = self.predict(x) ## 입력 데이터와 가중치 행렬의 결과값\n",
    "        return cross_entropy_error(y, t) ## 소프트맥스 함수의 출력값과 정답 레이블을 손실 함수를 통과 시킨다\n",
    "\n",
    "    ## 정확성\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1) ## y의 각 row에서 최대값을 갖는 인덱스 리턴\n",
    "        t = np.argmax(t, axis=1) ## t의 각 row에서 최대값을 갖는 인덱스 리턴\n",
    "\n",
    "        ### 각 row의 최대값 인덱스가 같은 개수 / 전체 row 개수 = 정확성\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    ## 기울기\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "\n",
    "        return grads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 신경망 학습 구현 실습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "## 하이퍼 파라미터\n",
    "iters_num = 10000 ## 반복 횟수\n",
    "train_size = x_train.shape[0] ## row\n",
    "batch_size = 100 # 미니 배치 크기\n",
    "learning_rate = 0.1 ## 학습률\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    ## 기울기\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    ## 매개변수 갱싱\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    ## 손실함수 그래프 저장용\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
