{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 오차제곱합 실습\n",
    "##### 오차제곱합의 수식의 결과값이 낮을 수록 정답에 가까울 확률이 높다는 것을 의미한다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "0.11949999999999998"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## y = 활성화 함수(소프트맥스)의 출력값\n",
    "y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0 ] # 정답이 2일 확률이 60%를 표현\n",
    "\n",
    "## t = 원-핫 인코딩 표기법: 하나의 원소만 1이고 나머지는 0으로 나타내는 표기법\n",
    "t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ] # 1원소가 2번 인덱스 이므로 정답이 2인것을 표현\n",
    "\n",
    "def sum_squares_error (y, t):\n",
    "    n = len(y)\n",
    "    sum = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        sum += np.sum( (y[i] - t[i]) ** 2 )\n",
    "\n",
    "    return sum / n\n",
    "\n",
    "sum_squares_error(np.array(y), np.array(t)) # -> 0.09750000000000003\n",
    "\n",
    "y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0 ] # 정답이 7일 확률이 60%를 표현\n",
    "sum_squares_error(np.array(y), np.array(t)) # -> 0.5975\n",
    "\n",
    "##### N개의 훈련 데이터를 오차제곱합의 수식 ######"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 교차 엔트로피 오차 실습\n",
    "##### 오차제곰합과 마찬가지로 결과값이 작을수록 정답일 확률이 높아진다는 의미\n",
    "##### 수식에는 없는 델타값은 더한 이유: np.log() 함수에 0을 입력하면 -inf(음의 무한대)가 되어 계산이 불가능해지기 때문에 아주 작은 값을 더해 음의 무한대 값을 피한다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4288846466.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [110]\u001B[0;36m\u001B[0m\n\u001B[0;31m    sum += -np.sum( ( t * np.log(y + d elta) ) )\u001B[0m\n\u001B[0m                                       ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cross_entropy_error(y, t):\n",
    "    n = len(y)\n",
    "    sum = 0\n",
    "    delta = 1e-7\n",
    "\n",
    "    for i in range(n):\n",
    "        sum += -np.sum( ( t * np.log(y + d elta) ) )\n",
    "\n",
    "    return sum / n\n",
    "\n",
    "y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0 ]\n",
    "t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ]\n",
    "cross_entropy_error(np.array(y), np.array(t)) ## -> 0.510825457099338\n",
    "\n",
    "y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0 ]\n",
    "cross_entropy_error(np.array(y), np.array(t)) ## -> 2.302584092994546"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 해석적 미분"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 순간 변화량을 표현하는 해석적 미분\n",
    "def numerical_diff(f, x):\n",
    "    ## 미분 에서 lim을 표현하기 위해 0보다 큰 가장 작은 값을 넣는다\n",
    "    ## 하지만, 이로 인해 계산의 결과에서 반올림 오차만큼의 오차가 발생하게 된다.\n",
    "    ## 이를 개선하기 위해 10^(-4)의 값을 h로 할당한다\n",
    "    h = 1e-4\n",
    "    m = (f(x + h) - f(x - h)) /(2 * h)\n",
    "    return m\n",
    "\n",
    "def b_numerical_diff(f, x):\n",
    "    h = 0.0001\n",
    "    return ( f(x + h) - f(x) ) / ( 2 * h )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def func_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "## retrun: x에 대한 f(x)의 변화량 -> 해석적 미분의 실제 기울기 값\n",
    "print(numerical_diff(func_1, 5))\n",
    "\n",
    "## return: x에 대한 f(x)의 변화량 -> 해석적 미분의 실제 기울기 값\n",
    "print(numerical_diff(func_1, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 편미분 실습"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def func_2(x):\n",
    "    return np.sum(x ** 2) ## x1^2 + x2^2\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0 * x0 + 4.0 ** 2.0\n",
    "\n",
    "def numerical_gradient(f, x): ## 기울기 구하기\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) ##\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "print(numerical_gradient(func_2, np.array([ 3.0, 4.0 ])))\n",
    "\n",
    "print(numerical_gradient(func_2, np.array([ 0.0, 2.0 ])))\n",
    "\n",
    "print(numerical_gradient(func_2, np.array([ 3.0, 0.0 ])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "X = np.arange(-6, 6, 0.1)\n",
    "Y = np.arange(-6, 6, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = np.sqrt(X ** 2, Y ** 2)\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='summer')\n",
    "ax.contourf(X, Y, Z, zdir='z', offset=0, cmap=plt.cm.summer)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 미니배치 학습 실습"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "## 기울기 함수\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "\n",
    "        return grad\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "\n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "\n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = func_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(func_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 경사 하강법"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 경사 하강법 함수\n",
    "def gradien_decent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x) ## 기울기 함수\n",
    "        x -= lr * grad ## 매계변수 - ( 학습률 * 기울기 )\n",
    "\n",
    "    return x, np.array(x_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## 경사법으로 f = x1^2 + x2^2의 최솟값을 구해라\n",
    "def func_2(x): ## x = [ x1, x2 ]\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([ -3.0, 4.0 ])\n",
    "x, x_history = gradien_decent(func_2, init_x, lr = 0.1)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 신경망에서의 기울기 구하기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "## 활성화 함수\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "## 손실 함수\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "## 기울기 함수\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) ## 정규분포 초기화 -> 랜덤 행렬\n",
    "\n",
    "    ## 입력된 x와 랜덤으로 생성된 가중치 행렬과의 곱으로 y값을 내보낸다\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    ## 손실 함수 ( 교차 엔트로피 오차 )\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) ## 입력신호 * 가중치\n",
    "        y = softmax(z) ## 활성화 함수를 통과한 확률 배열이 나온다\n",
    "        loss = cross_entropy_error(y, t) ## 손실 함수 통과\n",
    "        return loss ## 손실 함수의 결과값"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 신경망 생성\n",
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "X = np.array([ 0.6, 0.9 ])\n",
    "p = net.predict(X) ## 랜덤으로 생성된 가중치와의 곱으로 생성된 y값 행렬\n",
    "print(p)\n",
    "\n",
    "max = np.argmax(p) ## 결과 배열 중 최대값 요소의 인덱스 출력\n",
    "print(max)\n",
    "\n",
    "t = np.array([ 0, 0, 1 ]) ## 정답 레이블 생성\n",
    "net.loss(p, t) ## 손실 함수 통과 -> 0.928 ~"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def f(W):\n",
    "#     return net.loss(x, t)\n",
    "f = lambda w: net.loss(x, t) ## 간단한 표현식\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "## 해당 가중치 매개변수를 줄이기 위해서는 해당 기울기가 양수이면 음의 방향으로, 음수이면 양의 방향으로 이동해야함을 알 수 있다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 학습 알고리즘 구현 실습\n",
    "- 1단계: 미니 배치\n",
    "- 2단계: 기울기 산출\n",
    "- 3단계: 매개변수 갱신\n",
    "- 4단계: 1 ~ 3단계 반복"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 2층 신경망 생성\n",
    "# from common.functions import *\n",
    "# from common.gradient import numerical_gradient\n",
    "import numpy as np\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size) ## 인풋 행렬, 히든 행렬 사이즈 중간의 가중치 행렬 생성\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size) ## 은닉층 편향 행렬 생성\n",
    "\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size) ## 은닉층 행렬, 출력층 행렬 중간 사이즈 행렬 생성\n",
    "        self.params[\"b2\"] = np.zeros(output_size) ## 출력층 편향 행렬 생성\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        ## x = 입력 데이터\n",
    "        ## t = 정답 레이블\n",
    "        y = self.predict(x) ## 입력 데이터와 가중치 행렬의 결과값\n",
    "        return cross_entropy_error(y, t) ## 소프트맥스 함수의 출력값과 정답 레이블을 손실 함수를 통과 시킨다\n",
    "\n",
    "    ## 정확성\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1) ## y의 각 row에서 최대값을 갖는 인덱스 리턴\n",
    "        t = np.argmax(t, axis=1) ## t의 각 row에서 최대값을 갖는 인덱스 리턴\n",
    "\n",
    "        ### 각 row의 최대값 인덱스가 같은 개수 / 전체 row 개수 = 정확성\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    ## 기울기\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "\n",
    "        return grads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 신경망 학습 구현 실습"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "## 하이퍼 파라미터\n",
    "iters_num = 10000 ## 반복 횟수\n",
    "train_size = x_train.shape[0] ## row\n",
    "batch_size = 100 # 미니 배치 크기\n",
    "learning_rate = 0.1 ## 학습률\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    ## 기울기\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    ## 매개변수 갱싱\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    ## 손실함수 그래프 저장용\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}